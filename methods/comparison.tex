



\documentclass[11pt]{article}
% Default margins are too wide all the way around. I reset them here
\setlength{\topmargin}{-.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{.125in}
\setlength{\textwidth}{6.25in}

% Nicer paragraph style
\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{0pt}%

% Font
%\usepackage[urw-garamond]{mathdesign}
%\usepackage[T1]{fontenc}

\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lscape}

% Config
\graphicspath{{./images/}}
\hypersetup{
    colorlinks=true,
    bookmarks=true,
    unicode=true,
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    urlcolor=black
}

\begin{document}



\title{A Comparison Comparison:\\
\small{for comparing how to compare comparable things}}
\author{Stephen Wattam}
\date{\today}
\maketitle

% Table of contents
\tableofcontents{}
\pagebreak

% =============================================================================
\section{Introduction}
Comparison of corpora in an intuitive and meaningful manner has long been a problem facing corpus linguistics. The vast size (and, in some cases, complexity of annotation) of modern collections makes manual inspection of even a small fraction impossible, and any complex aggregation methods often rely on top-down imposition of structure or computationally intractable feature extraction.  Methods designed to reduce and summarise corpora %TODO: CITE
often apply their own bias, either  deliberate or otherwise.

Of prime interest to linguistic inquiry is the relative frequency of tokens in various corpora, and its formal partner, the relative probability that a token is included in a corpus. Various language models exist in order to more accurately determine the latter of these % TODO: CITE baroni's zipf, binomial approx to normal
, however, many of these require complex fitting stages to tweak their parameters (and produce variable performance even then).

Because of this complexity, comparison of large volumes of text at the token level remains a go-to tool of corpus linguistics. General frequency comparison methods (both for full texts as well as individual word frequencies) provides the linguist with a multitude of ways of exploring text, primarily (to the chagrin of some % TODO: CITE
) focusing on larger differences, or those on more frequent words.

Throughout the years, many methods for normalising word frequency for comparison have been developed, from within the fields of statistics (in which frequency comparison was its initial concern), psychology, artificial intelligence, information extraction, or linguistics itself.  This paper is an attempt to comprehensively cover these methods, their assumptions, applicability, properties, and application to real-world text in order to assess their suitability for use in comparing corpora in order to describe:
\begin{enumerate}
\item Their theoretical limitations (minimum frequencies, distributional assumptions, data format requirements);
\item Their variance given different real-world text distributions;
\item Their sensitivity to small but 'text-meaningful' changes (such as phraseology, word order, text type).
\end{enumerate}

In order for a comparison method to be featured here, it must satisfy the following criteria:
\begin{enumerate}
\item It must be non-parametric;
\item It must be practical to use on large volumes of text (with or without pre-processing);
\item It must describe notional similarity between two token-frequency lists. % DUH
\end{enumerate}

Not all of the methods analysed here will be entirely suitable, or intended, for its purpose of comparison.  Some metrics (\textsl{e.g.} Cronbach's $\alpha$) were never designed for the task of comparing token lists, whilst others will be poorly suited to the Zipfian distributions often seen in text.  We have included them for two reasons:
\begin{itemize}
\item Some are currently in use by those analysing text, without mention (or presumably consideration) of their failings.  We hope this discussion may lead to adoption of more suitable metrics;
\item Some exhibit properties and sensitivities that may prove useful as rules-of-thumb or rough guides for further investigation.  We aim to highlight these properties such that those participating in mixed methods research may capitalise upon them.
\end{itemize}


\paragraph{}
This paper is structured as a simple list of comparison algorithms, in the order, uhh, they are in my notebook. % TODO: make sections and group them as you go.
(They'll be grouped better later)
Possible groupings:
\begin{itemize}
    \item Rationale (separate PRE from correlation, prediction, agreement, disagreement);
    \item Field (stats, test/generalisability theory, psychology/psychometrics, information theory);
    \item Suitability (data type, necessary pre-processing);
    \item Computational tractability;
    \item Parsimony when applied to corpora.
    \item % TODO
\end{itemize}




\subsection{Notation}
\subsubsection{Lists and Crosstabulations}
%CITE: notation both semi-common and taken from http://publib.boulder.ibm.com/infocenter/spssstat/v20r0m0/index.jsp?topic=%2Fcom.ibm.spss.statistics.help%2Falg_crosstabs_somers.htm
Throughout the paper, we will be using the same formalism to describe frequencies of text in two input corpora, $X$ and $Y$, sorted in ascending order:
$$
X_1 < X_2 < ... < X_R
$$
and, accordingly for $Y$:
$$
Y_1 < Y_2 < ... < Y_C
$$

\begin{figure}[h]
\centering
  \includegraphics[draft,width=8cm,height=5cm]{crosstab.eps}%
  \caption{Crosstabulation of $X$ and $Y$}
  \label{fig:crosstab}
\end{figure}

For some measures, these will be crosstabulated into $R$ rows and $C$ columns, as illustrated in Figure~\ref{fig:crosstab}.  $f_{ij}$ represents the value at the $i$th column and $j$th row.  Marginal sums are given by 
$$ c_j = \sum_{i=1}^{R}f_{ij} $$ for the column total, and $$ r_i = \sum_{j=1}^{C}f_{ij} $$.  The grand sum is represented by $$S = \sum_{i=1}^{R}r_i = \sum_{j=1}^{C}c_i $$


\subsubsection{Pairs}
For measures that involve concordant or discordant pairs, they are defined as follows:
$$
pair = (X_i, Y_i), (X_j, Y_j);~~~ i \neq j
$$
Concordant pairs, $Q$, are such that a comparison between items $i$ and $j$ in the input lists yields the same sign:
$$
(X_i > X_j ~~\wedge~~ Y_i > Y_j)
~~~ \vee ~~~
(X_i < X_j ~~\wedge~~ Y_i < Y_j)
$$
Discordant pairs, $P$, exhibit differing signs.  Ties are counted where
$$
X_i = X_j ~~\vee~~ Y_i = Y_j
$$
The number of tied values is counted in $t_X$ for $X$, and, rather obviously, $t_Y$ for $Y$.
% TODO: move explanation of pairs up here, decide on something better than C, D, T for concordant, Discordant and Ties.
%Concordant pairs, denoted by $P$ are pairs of values such that % TODO

\subsubsection{Information Entropy}
% http://v8doc.sas.com/sashtml/stat/chap28/sect20.htm
Some metrics are defined in terms of the entropy of their lists, represented by $H$.  For rows:
$$
H(X) = -\sum_{i=1}^{R}{ \left[ \frac{ r_i }{ S }       \ln \left( \frac{ r_i }{ S } \right) \right] }
$$
accordingly for columns:
$$
H(Y) = -\sum_{j=1}^{C}{ \left[ \frac{ c_j }{ S }       \ln \left( \frac{ c_j }{ S } \right) \right] }
$$
And for joint entropy:
$$
H(X,Y) = -\sum_{i=1}^{R}{  \sum_{j=1}^{C}{ \left[    \frac{ f_{ij} }{S}  \ln \left( \frac{ f_{ij} }{ S } \right)     \right] } }
$$




%\begin{enumerate}
%\item As sets of numbers corresponding to ordered token frequency within each corpus.  In this case the total number of unique types is simply the counting measure of the set 
%    $$n_X = \mu(X)$$
%, and the total number of tokens its sum: 
%    $$m_X = \sum_{x \in X}^{} x$$
%\item As a contingency table formed from the sets $X$ and $Y$ as described above.  The grand sum, 
%    $$n_{\cdot \cdot} = n_X + n_Y$$ % TODO: should the subscript of N be something else?
%, with cell totals being defined in terms of their token:
%    $$ TODO $$ % TODO
%\end{enumerate}





%----

\subsection{Taxonomy}
Various methods of organising the methods in this paper prove both rational and useful; rather than organise the paper in one manner, denying those readers wishing to peruse it in others, we present a number of available taxonomies here for review.

% TODO


%----

\subsection{Evaluation}
Each of the measures is evaluated in the context of comparing two frequency lists, generated from text corpora, $X$ and $Y$, containing $R$ and $C$ token types respectively, and $S$ tokens collectively.

The properties of these measures of association will be judged for their usefulness when applied to problems of relating human ideas about text to corpus contents, namely:
\begin{itemize}
    \item Identification of relative homogeneity between corpora of varying speciality;
    \item Identification of relative heterogeniety between corpora of varying speciality;
    \item Small-scale identification of texts from different populations;
    \item TODO
    \item 
\end{itemize}





% =============================================================================
\section{Pearson's $\rho$} %name of group of statistics
\label{section:pearsonrho}
\subsection{Description}
Pearon's $\rho$ is one of the best-known (and oldest) measures of correlation, being devised by Karl Pearson (shortly after Sir Francis Galton's formalism of regression and correlation in the 1880's \cite{galton1888co}) %TODO: find a citation for Pearson's rho.  

It is a measure of correlation between two interval or rational quantites.  Pearson's $\rho$ is known for being sensitive to outliers, nonlinearity, heteroscedacity and anormality.

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [-1, 1] \\ \hline

    % Is the data typically input as a crosstabulation,
    % or as two parallel lists?
    Data type & interval \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions & linearity (Bivariate Normal) \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes, but sensitive \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
% How do we compute the statistic?
$$
\rho = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}
$$
Where $X$ and $Y$ are lists of the same length.

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?
$\rho$ is a measure of linear correlation (hence its roots), outputting 1 under conditions of perfect, positive linear association between $X$ and $Y$ (regardless of gradient), and -1 where the relationship is negative.  A value of 0 indicates linear independence.  

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
It's possible to achieve critical values by using permutation tests, bootstrapping, parametric tests using the normal distribution, or the Fisher transformation.
% =============================================================================







% =============================================================================
\section{Spearman's Rank $\rho$} %name of group of statistics
\subsection{Description}
% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?
Spearman's rank correlation coefficient is equivalent to Pearson's $\rho$ after transforming data into a ranked form.

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [-1, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions&  monotonicity \\ \hline

    % What types of data may this run on?
    Data type & ordinal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & Yes \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
% How do we compute the statistic?
The statistic is computed by assigning ascending rank to replace the values of $X$ and $Y$.  Any tied values are ranked and then reassigned a rank equal to the mean of their ranks: 

$$
\rho_s = \frac{\rho_{p_X} + \rho_{p_Y}}{2}
$$
Where $\rho_{p_i}$ is Pearson's $\rho$ of rank list $i$ (computed as in Section~\ref{section:pearsonrho}).


\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?
Spearman's $\rho$ is a measure of monotone correlation, outputting 1 where, for any two pairs of values $(X_i, Y_i)$ and $(X_j, Y_j)$, $X_i - X_j$ has the same sign as $Y_i - Y_j$.  It returns a value of -1 where there is no agreement between pair comparisons (and hence a consistent tendency for $Y$ to decrease as $X$ increases).  At a value of 0, there is no trend in $Y$ relative to the value of $X$ (50\% of pairs will have different signs), and the two are independent.

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
Similar to Pearson's measure, one can perform a permutatino test, or use modified Fisher transformation, or a t-test.  See also Page's trend test.
% =============================================================================









% =============================================================================
\section{Kendall's $\tau$} %name of group of statistics
\label{section:kendalltau}
\subsection{Description}
% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?
Kendall's Tau is a measure of rank association, measuring the same quantity as Spearman's $\rho$.  It comes in three forms:
\begin{enumerate}
\item $\tau_{a}$---Does not compensate for tied values, requires a square dataset;
\item $\tau_{b}$---Adjusts for tied values;
\item $\tau_{c}$---Adjusts for nonsquare input values (but not for ties).
\end{enumerate}
Goodman-Kruskal's $\gamma$ is recommended for data with large amounts of tied values.

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [-1, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions & monotonicity\\ \hline

    % What types of data may this run on?
    Data type & ordinal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & $\tau_a$, $\tau_b$ only \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
\label{section:kendalltau:formula}
% How do we compute the statistic?
The computation of Kendall's $\tau$ is based on the number of concordant pairs of values from $X$ and $Y$:

\subsubsection{$\tau_a$}
$$
\tau_a = \frac{Q-P}{0.5 n (n-1)}
$$

\subsubsection{$\tau_b$}
$$
\tau_b = \frac{Q-P}{ \sqrt{(Q + P + t_x)(Q + P + t_y)}  }
$$
{\color{red} some other forms for the denominator might be clearer for this}

\subsubsection{Stuart's $\tau_c$}
% http://v8doc.sas.com/sashtml/stat/chap28/sect20.htm has a slightly different formula, I think.
% TODO: check this.
$$
\tau_c = \frac{2(Q-P)}{n^2} = (Q-P) \left( \frac{ 2m }{n^2(m-1)} \right)
$$
{\color{red} some other forms for the above might be clearer for this}
Where $m = \min(R,~C)$ and $n$ is sample size.


\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?
The interpretation of Kendall's $\tau$ mirrors that of Spearman's rank correlation coefficient, it has value 1 if both rankings are identical, -1 if they are inverted, and 0 if they are independent.  Note that the way I explained the intrepertation of spearman's rank is the same way this is computed.

Values of -1 or 1 may only be achieved for square tables.


\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
Tau test.
% =============================================================================








% =============================================================================
\section{Goodman-Kruskal $\gamma$ (G)} %name of group of statistics
\subsection{Description}
Goodman-Kruskal's $\gamma$ is simply the difference between concordant and discordant pairs (computed as in Section~\ref{section:kendalltau}).  It is similar to Spearman's $\rho$ and Kendall's $\tau$, and is computed in a very similar manner to the latter.  $\gamma$ ignores ties.  Reduces to Yule's $Q$ for $2 \times 2$ tables.
% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [-1, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& monotonicity \\ \hline

    % What types of data may this run on?
    Data type & ordinal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & Yes \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
% How do we compute the statistic?
Using definitions of concordant and discordant pairs from Section~\ref{section:kendalltau:formula},
$$
\gamma = G = \frac{Q-P}{Q+P}
$$

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?
$\gamma$ is the surplus of concordant pairs, as a percentage of all pairs.  Consequently, it becomes 1 where all pairs agree, -1 where all pairs disagree in rank, and 0 where there is independence.

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% SEE http://publib.boulder.ibm.com/infocenter/spssstat/v20r0m0/index.jsp?topic=%2Fcom.ibm.spss.statistics.help%2Falg_crosstabs_somers.htm
% =============================================================================






% =============================================================================
\section{Cohen's $\kappa$} %name of group of statistics
\subsection{Description}
A measure of inter-annotator agreement between two raters.  It is commonly used in studies where annotators must mark up data in order to establish some measure of reliability for the resulting annotations.

It was introduced in 1960 \cite{cohen1960coefficient}.

Cohen's kappa and Scott's pi differ in terms of how Pr(e) is calculated.  Note similarity also to Fleiss' $\kappa$ % from wikipedia ;-)

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [0, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions & None (adjustment presumes random selection) \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes {\color{red}?} \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & Yes \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
% How do we compute the statistic?
$$
\kappa = \frac{P[a] - P[e]}{1-P[e]} =
$$
Where $P[a]$ is the probability of agreement between two observers:
$$
P[a] = \frac{ \sum_{i=1}^{R}f_{ii} }{ S }
$$
, and $P[e]$ is the hypothetical probability of random agreement, computed as the product of marginals over the grand sum:
$$
P[e] = \frac{ c_i r_j }{ S }
$$

\subsection{Intrepretation}
\label{section:cohenkappa:interpretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?
Cohen's $\kappa$ is an intra-class correlation coefficient, varying from 0 where there is no agreement to 1 where all categories contain identical values.  it is the proportion of agreement after chance agreement ($P[e]$) has been excluded. 

One problem with kappa is that the probability of items being in the wrong category, $P[e]$, is uniform across categories.  This is seldom a safe assumption (especially for human coders, who are more likely to choose similar categories than disparate ones when unsure).
{\color{red} cover failings in more detail (check kappash2.doc, others) !}

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
Rules of thumb are often used for $\kappa$, and it is possible to compute significance values, but they are seldom reliable due to factors discussed in Section~\ref{section:cohenkappa:interpretation}.
% =============================================================================





% =============================================================================
\section{Fleiss' $\kappa$} %name of group of statistics
\subsection{Description}
A generalisation of Scott's pi statistic %CITE: (Scott, W. (1955) pp. 321â€“325)
.  Measures inter-annotator agreement (similarly to Cohen's $\kappa$) between two or more raters.

Widely used in psychology.

\cite{fleiss1971measuring}
%  http://en.wikibooks.org/wiki/Algorithm_Implementation/Statistics/Fleiss'_kappa

%'Wikipedia: Fleiss' kappa specifically assumes that although there are a fixed number of raters (e.g., three), different items are rated by different individuals (Fleiss, 1971, p.378). That is, Item 1 is rated by Raters A, B, and C; but Item 2 could be rated by Raters D, E, and F.'
% http://www.statsdirect.com/help/agreement/kappa.htm

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [-1, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& 2 \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & ? \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & No \\ \hline
    
    % Is the method symmetric?
    Symmetry? & ? \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
% How do we compute the statistic?
$\kappa$ is defined in terms of the degree of agreement observed divided by the agreement possible by chance.
$$
\kappa_F = \frac{ \bar{P} - \bar{P_e} }{ 1 - \bar{P_e} }
$$

Partially, in terms of $p_j$, the proportion of entries that fall in column $j$
$$
p_j = \frac{ c_j }{S}
$$

$\bar{P_e}$ is computed as the sum of column marginals $p_j$, squared:
$$
\bar{P_e} = \sum_{j=0}^{C}{ p_j^2 }
$$

$P_i$ is the extent to which annotators agree across all columns (for row $i$):
$$
P_i = \frac{ 1 }{ n(n-1) }   \left( \sum_{j=1}^{C}{ f_{ij}^2 }  - n \right)
$$

Lastly, $\bar{P}$ is the mean of $P_i$:
$$
\bar{P} = \frac{ 1 }{ R }    \sum_{i=1}^{R}{ P_i }
$$


\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?
'It can be interpreted as expressing the extent to which the observed amount of agreement among raters exceeds what would be expected if all raters made their ratings completely randomly.' [wikipedia!]

\subsection{Critical Values}
Unknown, TODO
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================







% =============================================================================
% Proportional reduction in error
\section{Goodman \& Kruskal's $\lambda$}
\label{section:goodmankruskallambda}
\subsection{Description}
$\lambda$ is a measure of proportional reduction in error (as is Goodman \& Kruskal's $\tau$ and Theil's $U$, in Sections~\ref{section:goodmankruskaltau} and \ref{section:theilu} respectively).  First introduced in 1954 by those for which it is named~\cite{goodman1954measures}.

It is computed by using the modal value for one list as a predictor for the second, and as such requires a single modal value to be present (TODO: check).  It is available in both asymmetric and symmetric forms.

%Asymmetric and symmetric forms available.  Varies between 0 and 1, and is a measure of predictive capacity.  Suitable for nominal data.  Is a measure of proportional increase in predictive accuracy, as is Goodman and Kruskal's $\tau$ and the Theil's U (uncertainty coefficient).   Note that $\lambda$ suffers from uniqueness assumptions in establishing the mode.  There seems to be no single method to escape these, but check 
% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [0, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& Single modal value \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & No \\ \hline
    
    % Is the method symmetric?
    Symmetry? &  Optional \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
% SEE: http://vassarstats.net/lamexp.html FOR AN AMAZING EXPLANATION!
\subsubsection{Asymmetric}
(with $Y$ as the dependent variable):
$$
\lambda(Y|X) = \frac{ \varepsilon_1 - \varepsilon_2 }{ \varepsilon_1 }
$$
Where $\varepsilon_1$ is the sum of marginal frequencies for each row (independent, $X$) minus the sum of the row with the modal frequency, $r_{mode}$:
$$
\varepsilon_1 = \sum_{i=0, i \neq mode}^{R}{ r_i }
$$
and $\varepsilon_2$ is the sum of all modal row categories (for which $f_{ij}$ is greatest) in each row:
$$
\varepsilon_2 = \sum_{i=0}^{R}{ \max( f_{i\cdot} ) }
$$

\subsubsection{Symmetric}
the average of two asymmetric lambdas:
% Find formula from http://v8doc.sas.com/sashtml/stat/chap28/sect20.htm
$$
\lambda = \frac{ \lambda(Y|X) + \lambda(X|Y) }{ 2 }
$$


% How do we compute the statistic?

\subsection{Intrepretation}
$\lambda$ takes the value 0 where two values are in accord, and 1 where the variables differ.  This distinction is made by comparing modal categories, and is not concerned by precise frequencies or proportions.
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
Unknown, TODO.
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================








% =============================================================================
%
\section{Theil's $U$ (Uncertainty Coefficient)}
\label{section:theilu}
\subsection{Description}
Another measure of proportional reduction in error, this measure is based on the entropy of its input lists, and is a normalised form of Mutual Information.  This means it considers the whole distribution, rather than simply the mode (as with $\lambda$, Section~\ref{section:goodmankruskallambda}).  

It was first defined in 1966 by Henri Theil~\cite{theil1966applied}.  It is widely used in information retrieval, evaluating clustering algorithms, and many other scientific fields.

Theil's $U$ is available in both symmetric and asymmetric forms.

%Available in both symmetric and asymmetric forms.  Measures reduction in error when the independent variable is used to measure the dependent.  Suitable for nominal data (can be applied to continuous using density estimation).  Based on the concept of information entropy, and is essentially a normalised form of Mutual Information---thus useful for assessing information based systems like clustering algorithms.  Unlike $\lambda$, considers whole distribution rather than simply the mode.  Used for evaluating the quality of classifiers.  Ranges from 0 to 1.

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [0, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& None \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & No \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Optional \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
Since Theil's $U$ is a normalised form of mutual information, it is pertinent to define it:
$$
MI(X,Y) = H(X) + H(Y) - H(X, Y)
$$

\subsubsection{Asymmetric}
With $Y$ as the dependent variable, we can define $U$ in terms of mutual information:
$$
U(Y|X) = \frac{ MI(X,Y) }{ H(Y) }
$$

\subsubsection{Symmetric}
% NB: this is a simplification of two of the above, with the denominators summed too.
$$
U = 2 \left[ \frac{ MI(X,Y) }{ H(X) + H(Y) } \right]
$$


% How do we compute the statistic?

\subsection{Intrepretation}
Like other measures of proportional reduction in error, this measure is 0 for perfectly accordant variables, and 1 for entirely discordant ones.  To achieve its extremes, both variables must be identical in proportion, but not frequency. {\color{red} check and elaborate}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
Unknown, TODO
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================












% =============================================================================
\section{Goodman \& Kruskal's $\tau$}
\label{section:goodmankruskaltau}
\subsection{Description}
Another measure of proportional reduction in error, as with Theil's $U$ (Section~\ref{section:theilu}) and Goodman \& Kruskal's $o\lambda$ (Section~\ref{goodmankruskallambda}).  First introduced in 1954, along with Goodman and Kruskal's other measures \cite{goodman1954measures}.
According to Goodman and Kruskal, it is to be interpreted as the ``relative decrease in the proportion of incorrect predictions when we go from predicting the row category based only on the row marginal probabilities (as in Lambda) to predicting the row category based on the conditional proportions of both row and column.''

% Measures proportional reduction in error, as the two above do.  
% Used for nominal data. 
%Sensitive to margin differences.  
%Asymmetric (as with the other PRE statistics).  
% Produces the same results as $\phi^2$ for 2x2 tables.  
% Ranges from 0-1, 0 being unassociated, 1 being entirely associated.  
% Introduced in 1954~\cite{goodman1954measures}.




% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [0, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& ? \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & No \\ \hline
    
    % Is the method symmetric?
    Symmetry? & No \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
$\tau_{GK}$ is a PRE measure, and so is a ratio of predictive capacity in the data (whilst considering both variables) to the marginal sums:
$$
\tau_{GK}(Y|X) = \frac{   S \sum_{i, j}{ \frac{ f_{ij}^2 }{ r_i } } - \sum_{j=1}^{C}{ c_j^2 }    }{   S^2 - \sum_{j=1}^{C}{ c_j^2 }    }
$$
% How do we compute the statistic?

\subsection{Intrepretation}
Sensitive to differences in marginal values.  Takes the value 0 where there is no association, and 1 where both marginal sums are entirely in accord.

For $2\times 2$ tables, this measure is identical to $\phi^2$ (see Section~\ref{section:phi}).
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
Unknown, TODO
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================







% =============================================================================
\section{Kullback-Leibler Divergence}
\subsection{Description}
Available in both symmetric and asymmetric forms.  Measures the difference between two probability distributions in terms of the extra information in the depdenent one.  Computed from Fisher information.  

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [0, ? TODO] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& None \\ \hline

    % What types of data may this run on?
    Data type & nominal, ordinal, interval, ratio ? TODO \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & Yes \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Optional \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
\subsubsection{Asymmetric}
$$
D_{KL}(Y|X) = \sum_{i=1}^{R}{ \ln \left( \frac{X_i}{Y_i} \right) } X_i
$$
Where $R$ is, as in the Notation section, the length of both $X$ and $Y$ (due to squareness assumptions).

\subsubsection{Symmetric}
Kullback and Leibler defined their divergeance symmetrically, as the sum of two asymmetric divergeances:
$$
D_{KL} = D_{KL}(Y|X) + D_{KL}(X|Y)
$$

% How do we compute the statistic?

\subsection{Intrepretation}
Describes the amount of information `lost' when using $Y$ to approximate $X$, measured in bits.  Unlike most formulae covered here, the output of the divergeance is in the same dimension as its input.  Takes the value 0 when both distributions are identical.
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
Unknown, TODO
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================








% =============================================================================
\section{Somers' $D$}
\label{section:somersd}
\subsection{Description}
Somers' $D$ comes in both asymmetric and symmetric forms.  It is used in psychology and geography, and was first defined in 1962~\cite{somers1962new}.
% Suitable for ordinal data.  
% Ranges from -1 to 1, -1 being a negative relationship.  
The asymmetric form is an extension of gamma, differing only in the inclusion of the number of pairs not tied on the independent variable.  
Somers's $D$ is an asymmetric modification of $\tau_b$ (note: the paper mentioned below says that it is defined in terms of $\tau_a$, see below), in that it only corrects for pairs on the independent variable.  
% Appropriate only for when both variables are ordinal.  
Related to Harrell's c index (Harrell \textsl{et al.} 1982/1996) as $D = 2c - 1$.

TODO: find where it was first used or defined, cite Harrell.

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [-1, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions & None \\ \hline

    % What types of data may this run on?
    Data type & ordinal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & Yes (with 0-padding) \\ \hline
    
    % Is the method symmetric?
    Symmetry? & No \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
\subsubsection{Asymmetric}
Somers' $D$ is defined in terms of concordant and discordant pairs, as with Kendall's $\tau$:
% http://srmo.sagepub.com/view/the-sage-dictionary-of-statistics/n537.xml
$$
D(Y|X) = \frac{ P - Q }{ P + Q + t_X }
$$

Newson~\cite{newson2006confidence} also defines $D$ in terms of Kendall's $\tau_a$:
$$
D(Y|X) = \frac{ \tau_a(XY) }{  \tau_a(XX) }
$$

\subsubsection{Symmetric}
$$
D = \frac{ P - Q } { P + Q + \frac{ t_X + t_Y }{ 2 } }
$$

% How do we compute the statistic?

\subsection{Intrepretation}
Takes the value of -1 when describing a perfect negative relationship, 0 when independent, and 1 for perfect accordance.

% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
Unknown, TODO
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================








% =============================================================================
\section{$\eta$}
\label{section:eta}
\subsection{Description}
Appropriate for `nominal by interval' data, that is dependent measured on interval scale, independent on nominal scale.  Used in psychology.  
% Asymmetric.  
% Does not assume linearity.  
TODO: find origin of eta (citation)

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [0, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& None \\ \hline

    % What types of data may this run on?
    Data type & nominal (independent) interval (dependent) \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & No \\ \hline
    
    % Is the method symmetric?
    Symmetry? & No \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
% from http://publib.boulder.ibm.com/infocenter/spssstat/v20r0m0/index.jsp?topic=%2Fcom.ibm.spss.statistics.help%2Fidh_xtab_statistics.htm
% Two eta values are computed: one treats the row variable as the interval variable, and the other treats the column variable as the interval variable.

Eta is computed as a ratio of two sums of squares.  The former of these is the sum of square differences between the row means and the grand mean cell value $f_\mu = \frac{S}{RC}$:
$$
F_{between} = \sum_{i=1}^{R}{    \left(   f_\mu -   \frac {r_i}{C} \right)^2  }
$$

The latter is the total sum of squares, taken from each cell:
$$
F_{total} = \sum_{i=1}^{R}{ \sum_{j=1}^{C}{   ( f_\mu - f_{ij} )^2    } }
$$

$\eta$ is simply the root of the ratio of $F_{between}$ and $F_{total}$:
$$
\eta(Y|X) = \sqrt{  \frac{ F_{between} }{ F_{total} }   }
$$
% How do we compute the statistic?

\subsection{Intrepretation}
Eta takes the value 0 where no association is found between $X$ and $Y$, and 1 where both variables are monotonically related.  Linearity is not assumed.

$\eta^2$ represents the proportion of variance in the dependent variable explained by the independent.
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
Unknown, TODO
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================









% =============================================================================
% TODO vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
\section{Pearson $\chi^2$}
\label{section:pearsonchisq}
\subsection{Description}
Defined in 1900 by Karl Pearson~\cite{pearson1900x}.
Mention $\phi$ and Cram\'er's $V$, and the Contingency coefficient (based on chi-square).

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & $[0, \infty] \to \chi^2$  \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions & Normal {\color{red} ?} \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & No \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & No \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
The test statistic is normally defined in terms of observed and expected frequencies, typically computed from marginals:
$$
\chi^2 = \sum_{i=1}^R{  \sum_{j=1}^C{ \frac{ (f_{ij} - \mathbb{E}_{ij} )^2 }{ \mathbb{E}_{ij} } } }
$$
where $\mathbb{E}$ for cell $(i,j)$ is computed as:
$$
\mathbb{E}_{ij} = \frac{ c_{i \cdot} r_{\cdot j} }{ S }
$$

% How do we compute the statistic?

\subsection{Intrepretation}
The value of the $\chi^2$ statistic follows...
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================













% =============================================================================
\section{$\phi$, (Matthews Correlation Coefficient)}
\label{section:phi}
\subsection{Description}
An attempt to remove effects of sample size from $\chi^2$.  
Symmetric. 
Suitable for use on nominal data (due to chisq link). % CITE PPT file (https://docs.google.com/viewer?a=v&q=cache:ZZMuGFKtHVUJ:www.stanford.edu/class/archive/anthsci/anthsci192/anthsci192.1064/handouts/6%2520contingency%2520tables.ppt+&hl=en&gl=uk&pid=bl&srcid=ADGEESiGTqYGMt9IpeVbC4volI_SsuNVvjvaf1lZ0vKM1jomiVOtr4D6gacKD-l_AV9ClME6xulbFlmUOlsz_Ptmh_h1WDUxTYGitB66xgaEzkes9g3pR44dY0_RLhVYHMzDCYbPGUjW&sig=AHIEtbRRr2n4NXo1muTnJsJmN5BWw_W5-g)
Ranges from 0-1 for 2x2 tables, with no upper limit for larger ones.  
Identical in concept to to Pearson's $\rho$ {\color{red} Does this \textbf{always} output the same result?}.

Mathews' correlation coefficient was designed for use with classifiers' confusion matrices, and is essentially the same form expressed in terms of classifier output (accuracy, precision, recall \textsl{etc.}). 
% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & Depends on table size: $[-1, 1]$ for $2\times 2$, $[-1, \infty]$ general \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& 2 \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & No \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & No \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
$$
\phi = \sqrt{ \frac{\chi^2}{S} }
$$
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================












% =============================================================================
\section{Cram\'er's $V$}
\label{section:cramersv}
\subsection{Description}
A standardisation of $\phi^2$ (Section~\ref{section:phi}).  
Ranges from 0 to 1, 0 being no association, 1 being complete.  
Identical to Tschuprow's T for square tables.  
Cram\'er's $V$ doesn't require a square table.  
Suitable for use on nominal data (due to chisq link).
% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [0, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& 2 \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & No \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & No \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
$$
V = \sqrt{ \frac{ \chi^2 }{ S( m - 1 )} }
$$
Where $m$ is the smaller of the two table dimensions, $\inf(R, C)$  (and $S$ is my convention, the number of cases/grand sum).
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================















% =============================================================================
\section{Pearson's Contingency Coefficient}
\label{section:contingencycoefficient}
\subsection{Description}
Another statistic that corrects $\chi^2$ for non-2x2 table sizes.  
As such it's used on nominal data.  
Maximum value depends on the size of the table, being 0.707 for 2x2 tables, 0.87 for a 4x4 table etc.  
Scale invariant for $S$, but not for $R$ or $C$ (sensitive to table size changes but not sample size).
% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [0, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& 2 \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & No \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & 8 \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
% http://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/pearcont.htm
$$
CC = \sqrt{ \frac{ \chi^2 }{ \chi^2 + S } }
$$
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================














% =============================================================================
\section{Odds Ratio}
\label{section:oddsratio}
\subsection{Description}
A measure of an increase in $Y$ given an increase in $X$. % TODO: find original explanation
Usually computed only for 2x2 tables, it's possible to extend odds ratio computations by generating many 2x2 contingency tables (category $k$ vs. not category $k$).  Suitable for nominal data.  % http://www.john-uebersax.com/stat/odds.htm#extens loglinear association recommended instead in multi-category case
Log odds most often used instead, making for simple confidence interval computations.  
It's computed as the ratio of the odds of something occurring in one category to the odds of occurrance in another category.  
Range of $[0-\infty)$, above one is positive relationship, below 1 is negative association, 1 is no association (log odds normalises this to $[-\infty, \infty]$ around 0 as the centre-point).  
Symmetric.  
For an $R$ by $C$ table, there are $(R-1)(C-1)$ non-redundant odds ratios.  
Log-odds is slightly more conservative than Yule's Q (Section~\ref{section:yulesq}).



\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & $[0, \infty]$ `raw' (TODO: Ahrg, stupid memory, what is this called!?), $[-\infty, \infty]$ logged \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& ? \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & No {\color{red} hmm check this for the context} \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & 8 \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
The formula for odds ratios in a $2\times 2$ context is (in this case, ratio of row 1 to row 2):
$$
OR = {   \frac{ f_{1\cdot} }{ (1-f_{1\cdot}) }   \bigg /  \frac{ f_{2\cdot} }{ (1-f_{2\cdot}) }        }
$$

For a $2\times 2$ matrix this becomes:
$$
OR = { \dfrac{f_{22}/(f_{22}+f_{21})}{f_{21}/(f_{22}+f_{21})} \bigg / \dfrac{f_{12}/(f_{12}+f_{11})}{f_{11}/(f_{12}+f_{11})}} = \dfrac{f_{22}f_{11}}{f_{21}f_{12}}
$$


Or, for larger matrices (applied repeatedly):
FIXME: I think this should be applied using a different algorithm to those mentioned in my sources, which compute local odds ratios.  
See Mantel-Haenszel, it's just the average of odds ratios, but the notation might get ugly with contingency table style $f_{ij}$.
% http://pages.uoregon.edu/aarong/teaching/G4075_Outline/node20.html
%$$
%OR = \dfrac{f_{kk}f_{\not k\not k}}{f_{k \not k}f_{\not k k}}
%$$
% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================













% =============================================================================
\section{Yule's $Q$}
\label{section:yulesq}
\subsection{Description}
This is a normalisation of the odds ratio in Section~\ref{section:oddsratio} to vary between -1 and 1.  
See also Log Odds ratios, which do a similar thing (making the OR vary around 0).




% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [-1, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& 2 \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & No \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & No \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
$$
Q = \frac{OR - 1}{OR + 1}
$$
Where $OR$ is the odds ratio (see Section~\ref{section:oddsratio}) of the data.
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================













\section{Cochran's Statistic}
See Section \ref{section:mantelhaenszel}.  Cochran's statistic is merely an uncorrected form.

% =============================================================================
\section{Mantel-Haenszel}
\label{section:mantelhaenszel}
\subsection{Description}
Tests conditional independence of two binary values, controlling for one or more categorical variables.  
Requires ordinal data {\color{red} Why? If this is simply a load of means of odds ratios...}.  
Based on odds ratios (Section~\ref{section:oddsratio}).  
First defined in 1959 {\color{red} (though the only refernece I have thus far is from '04...)}~\cite{mantel2004statistical}.  
Computed once after each 2x2 group (for $k$ groups) has itself been computed.  
Assumes squareness, but items could be replaced with 0s.
Mantel-Haenszel's statistic is a corrected form of Cochran's, with small-sample correction and inflation of variance.
% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [-1, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& 2 \\ \hline

    % What types of data may this run on?
    Data type & nominal, ordinal, interval, ratio \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & 8 \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & Yes \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
% FROM http://www.quantitativeskills.com/sisa/statistics/two2hlp.htm#MHX
% Mantel Heanszel Chi-square Mantel-Haenszel Chi-square is thought to be closer to the 'true' Chi-square if small numbers of cases are involved.

Mantel-Haenszel's test is designed to be used on a series of pooled $2\times 2$ tables, for example, from different locations, combining the odds ratios into a single statistic.  Applied to multiple categories, it would be possible to form a $2\times 2$ confusion matrix for each category.

\subsubsection{Mantel-Haenszel Test Statistic}
The test statistic follows a chi-squared distribution and is:
$$
\chi^2_{MH} = \frac{ A^2 }{ Var(A) }
$$
where
$$
A = \sum_{i=1}^{R}{  \frac{  X_i Â¬Y_i  -  Â¬X_i Y_i }{ S }  }
$$
where $Â¬X_i$ is the count of items not in category $i$ for variable $X$ (\textsl{i.e.} $\sum{X} - X_i$), and:
$$
Var(A) = \frac{   \sum_{i=1}^{R}{  R C (X_i Y_i) (Â¬X_i Â¬Y_i) }  }{  S^2(S - 1)  }
$$
Note that, for repeated categories, $S$ is the grand total (as in Notation), but otherwise may vary per stratum to be the total of items in that straum only.

TODO: mention the statistic varying with chisq, how many DOF?

\subsubsection{Mantel-Haenszel Odds Ratio}
This is simply the weighted average of the odds ratios from each stratum.  Note that, where the totals of each stratum are equal (i.e. during artificial stratification for $k$ groups), this is simply the average of odds ratios.  {\color{red} I should probably omit this whole thing in that case...}.
$$
OR_{MH} = \frac{ { \sum_{i=0}^{R}{ X_i Â¬Y_i } / S } }{ { \sum_{i=0}^{R}{ Â¬X_i Y_i } / S} }
$$
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================















% =============================================================================
\section{Cronbach's $\alpha$}
\label{section:cronbachalpha}
\subsection{Description}
Commonly used as a measure of reliability/`internal consistency' of psychometric tests \cite{schmitt1996uses} (it was originally designed for this too).  
First named $\alpha$ in 1951~\cite{cronbach1951coefficient}, and is an extension of the Kuder-Richardson Formula 20, an equivalent measure for binary data( \textsl{i.e.} $2\times 2$ tables).
Related to the Spearman-Brown prediction formula, and factor analysis.  
Varies from 0 to 1 normally, or, strictly, in the range $(-\infty, 1]$, though according to Ritter~\cite{ritter2010understanding}, can vary below 0~in some circumstances {\color{red} (which? this \cite{cronbachalphanegative} )}. \cite{bland1997statistics}
% http://srmo.sagepub.com/view/the-sage-dictionary-of-statistics/n13.xml
% http://www.ats.ucla.edu/stat/spss/faq/alpha.html

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & $(-\infty, 1]$, typically remains above 0 \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& 2 \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency &  \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions &  \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
Cronbach's $\alpha$ is generally used to test agreement between scales, and as such is often phrased in terms of tests and scores.  
For these formulae, each row will be one `question' or `testlet', with the columns representing multiple scores for each.  
In this case, the numerator represents the sum of row variances, whilst the denominator is the total variance.
$$
\alpha = \frac{ R }{ R - 1 }   \left( 1 - \frac{ \sum_{i=1}^{R}{ \sigma_{f_{i\cdot}}^2 }    }{  \sigma_{S}^2  } \right) 
$$
A different form is written thus:
$$
\alpha = \frac{ R\bar{c} }{ \bar{v} + (R - 1)\bar{c} }
$$
Where $\bar{v}$ is the mean variance for each row, and $\bar{c}$ is the mean covariance between the components across the 'current sample of persons' {\color{red} Does this mean columns?}
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================














% =============================================================================
\section{Tschuprow's $T$}
\label{section:tschuprowt}
\subsection{Description}
For nominal data.  
Range between 0 and 1, 0 where no association, 1 where full association.  
Can only achieve 1 for square tables. 
Identical to Cram\'er's $V$ for square tables.  
Based on $\phi^2$ (Section~\ref{section:phi}).
% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [0, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& 2 \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & No \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & No, but changes limits \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
$$
T = \sqrt{ \frac{ \phi^2 }{ \sqrt{ (R-1)(C-1) } } }
$$
Where 
$$
\phi^2 = \frac{\chi^2}{n}
$$
as in Section~\ref{section:phi}
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================

















% =============================================================================
\section{$F_\beta$-score}
\label{section:fscore}
\subsection{Description}
Applicable as classifier output, hence binary data (confusion matrix).  
May be extended by applying in turn to each category, as with other methods here.  
Represents the capacity of the independent variable as a predictor of the dependent.  
May be weighted (by changing $\beta$) to change importance of precision or recall.  
Does not take into account true negative rate (which is arguably good for text comparison).
Based on van Rijsbergen's effectiveness measure.
Introduced in 1979~\cite{Rijsbergen1979IR539927}.

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & ? \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& 2 \\ \hline

    % What types of data may this run on?
    Data type & nominal, ordinal, interval, ratio \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & 8 \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & 8 \\ \hline
    
    % Is the method symmetric?
    Symmetry? & ? \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
$F_\beta$ is defined in terms of a confusion matrix, which, mapped onto a $2\times 2$ contingency table, resolves to:
$$
precision = \frac{ f_{11} }{ f_{11} + f_{12} }
$$
and
$$
recall = \frac{ f_{11} }{ f_{11} + f_{21} }
$$
where the table is organised such that $(1,1)$ contains counts of `true positives`, that is where both $X$ and $Y$ hold values for category $i$ (tp, fp, \ fn, tn).

From these, the general formula for $F_\beta$ is:
$$
F_\beta = (1 + \beta^2) \frac{ precision ~\cdot~ recall }{ (\beta^2 ~ \cdot ~ precision) + recall }
$$
Where $\beta$ defines the importance of recall relative to precision (twice as important where $\beta = 2$).

TODO: write form for use with non-2x2 types.
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================













% =============================================================================
\section{Mann-Whitney-Wilcoxon $U$}
\label{section:mannwhitney}
\subsection{Description}
Used for hypothesis testing.  
Large samples see $U$ to be normally distributed.  
Equivalent to Kendall's $\tau$ where one variable is binary.  
suitable for use with ordinal data.  
Works on ranked data.  
Works by ranking data and then inspecting the distribution of each input list within the rankings.  
$U$ follows the normal distribution, and is more of a formal test statistic than many on this list, so technically its limits are $[-\infty, \infty]$.  
Is a variant of the Wilcoxon signed-rank test without the assumption that both input variables come from the same population.

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & $[\infty, \infty] \to Normal(?, ?)$ \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions & monotonicity \\ \hline

    % What types of data may this run on?
    Data type & ordinal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & No \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & No \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
First, frequency lists $X$ and $Y$ are combined into a single list, $N$ (the information regarding which point is from which list is used later).  $N$ is then replaced with its rank (including means for ties, as in Section~\ref{section:spearman}).

$U$ is computed on either list, and is the proportion of ranks that appear within the list, relative to the size of the list itself.  If this is 0.5, we know the data is randomly distributed:
$$
U_X = RC + \frac{ R(R+1) }{ 2 } - \sum_{i=0}^{R}{ rank(X_i) }
$$
where $R$ is the length of $X$, as in the notation section.  The same statistic may also be computed for $Y$, and it is conventional to use the shorter of the two lists.

\subsubsection{Critical Values}
For $R+C > 10$, $U$ follows a Normal distribution with standard deviation:
$$
\sigma = \sqrt{  \frac{  RC(R + C + 1)  }{  12 } }
$$
For small values of $R$ or $C$, the approximation is invalid.
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================














% =============================================================================
\section{Kolmogorov-Smirnov's $D$}
\label{section:kolmorogorvsmirnovd}
\subsection{Description}
Used to compare frequency distributions.  
Works by taking the maximum distance between the cumulative distribution of two variables.  
Requires ordinal data.  
Unaffected by scale changes, but then most of these probably are.  
Requires squareness.  
$D_{KS}$'s limits can be looked up in critical value tables for a given sample size (though this assumes normality {\color{red} check that}, but its limits are defined in terms of the original data.

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & Greatest category value: $[0, \sup(X, Y)]$ \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions & None \\ \hline

    % What types of data may this run on?
    Data type & ordinal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & No, but 0-padding will change score \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
Computed as the proportion of the difference in cumulative distribution between $X$ and $Y$.
$$
D_{KS} = \sup_{i=1}^{R}\left[ \frac{ X_i }{ \sum{X} } - \frac{Y_i}{ \sum{Y} } \right]
$$
Note that $X$ and $Y$ are ordered, as in the Notation section, and due to the squareness requirement $R$ is the length of both $X$ and $Y$.
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================
















% =============================================================================
\section{Log-Likelihood Ratio}
\label{section:loglikelihood}
\subsection{Description}
Suitable for use on nominal data.  
$\Lambda$ used to represent.  
Can be used to compare two arbitrary models based on their likelihoods.  
Used for the Wilks test with contingency tables because $-2ln(\Lambda) \to \chi^2$ as $S \to \infty$ (with d.f. equal to the difference in dimensionality between null and alternative models, in thic case $|R-C|$.)  
Likelihood ratios must be above a given threshold to indicate significant differences between models, but they vary within the range $[0, 1]$---at 0, the alternative model was not at all possible (no agreement), at 1 it was as likely to occur as the null model (full agreement).

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & $[0, \infty] \to \chi^2$ \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions&  \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & No \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & Yes (0-padding) \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
% From http://www.quantitativeskills.com/sisa/statistics/two2hlp.htm#LRX
% The LRX has the important property that an LRX with more than one degree of freedom can be partialised into a number of smaller tables each with its own (smaller) LRX and (lower numbers of) degrees of freedom. The sum of the partial LRXs and associated partial degrees of freedom, as found in the smaller tables, equals the original LRX and original number of degrees of freedom.

The general model comparison may be written as:
$$
D = -2ln \left( \frac{ likelihood~of~null~model }{ likelihood~of~alternative~model } \right)
$$

\subsubsection{Wilks' Statistic}
Performed on contingency tables (for computation of the Wilks' statistic $W$):
$$
W = -2\ln\Lambda = 2 \sum_{i=1}^{R}{ \sum_{j=1}^{C}{ f_{ij}\ln \left( { \frac{f_{ij}}{r_i}  \bigg/  \frac{c_j}{S}  }  \right)   }}
$$
% See http://ucrel.lancs.ac.uk/llwizard.html for handy decomposure of the above, the denominator is really an expected value for the call frequency.
% It might be handy to make that into another equation section \mathbb{E}...
For comparison against a $\chi^2$ distribution with $d.f.~= |R-C|$.


\subsubsection{$\Lambda$}
To compute `straight' Likelihood ratio $\Lambda$:
$$
\Lambda = - \frac{ e^W }{ 2 }
$$
TODO: perhaps try to phrase this without going via Wilks', it may make the computation easier or less lossy.


% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================
























% =============================================================================
\section{Cosine Similarity}
\label{section:cosine}
\subsection{Description}
A measure of angular similarity using a vector space model.  
Often used in data/text mining.  
Ranges from -1 to 1, with -1 being precise negative relationship and 1 being identical data (0 is independence).  
Known in biology by the names ``Ochai coefficient'' or ``Ochiai-Barkman coefficient''.  
Assumes squareness and is sensitive to ordering (ordinal).
% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [-1, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions&  \\ \hline

    % What types of data may this run on?
    Data type & ordinal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & Yes \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
May be computed using the euclidean dot product of two vectors.  See also euclidean distance, Manhattan distance below (Sections~\ref{section:euclid}\ref{section:manhattan} respectively).

It may be computed thus:
$$
cos = \frac{ X \cdot Y }{ \|X\|\|Y\| } = \frac{  \sum_{i=0}^{R}{ X_iY_i }  }{     \sqrt{ \sum_{i=1}^{R}{X_i^2} }  \cdot  \sqrt{ \sum_{i=1}^{R}{Y_i^2} }    }
$$
Note that $R$ may be used to represent the length of $X$ and $Y$ due to squareness requirement.

% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================


















% Distances
% =============================================================================
\section{Percentage Score (Normalised) Difference}
\label{section:percentage}
\subsection{Description}
The percentage of an entry in $X$, minus the percentage of an entry in $Y$.  
Asymmetric unless the sign is deliberately discarded {\color{red} is this done often?}.  
Doesn't adjust for the probability of association by chance.  
Assumes squareness in this form (could be padded with 0s).  
Varies in the range $[0-100]$ in the \% age form, where 0\% is perfect similarity of proportions, and 100\% is total difference.
% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [-100, 100] asymmetric, [0, 100] symmetric\\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& None \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & Yes (0-padding rational) \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Optional \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
This is simply the difference in proportions each corresponding item in lists $X$ and $Y$:

\subsubsection{Asymmetric}
$$
D_\%(Y|X) = { 100 \sum_{i=0}^{R}{ \left(  \frac{X_i}{\sum{X}} - \frac{Y_i}{\sum{Y}} \right) }  \bigg / R }
$$
Clearly, divide this by 100 to get a scale from 0-1.  $R$ is used above under squareness assumptions that it represents the length of $X$ and $Y$.

\subsubsection{Symmetric}
$$
D_\%(Y|X) = { 100 \sum_{i=0}^{R}{ \left|  \frac{X_i}{\sum{X}} - \frac{Y_i}{\sum{Y}} \right| }  \bigg / R }
$$
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================














% =============================================================================
\section{Euclidean Distance}
\label{section:euclid}
\subsection{Description}
The distance between two points in vector-space as travelled directly.  
Uses pythagoras to determine `direct route'.  
Its limits are dependent on the data, with maximum association at 0 and minimum/negative association at TODO...  
As with other vector-space methods, requires squareness.  
Asymmetric, but only the sign varies (as above)
{\color{red} should I make this into two formulae as above? probably, yes. TODO}
% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & umm, $\pm (\sum{X} + \sum{Y}) $ I think. \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& None \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & Yes (0-padding possible) \\ \hline
    
    % Is the method symmetric?
    Symmetry? & No (sign varies) \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}

\subsubsection{Asymmetric}
$$
D_E = \sqrt{ \sum_{i=1}^{R}{ (Y_i - X_i)^2 } }
$$
Where $R$ is the number of items in $X$ and $Y$, as per Notation above.

\subsubsection{Symmetric}
 TODO!
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================













% =============================================================================
\section{Manhattan/Taxicab Distance}
\label{section:manhattan}
\subsection{Description}
Also known as squared Euclidiean distance, for obvious reasons.  
Equivalent to travelling between two points in vector space by moving only orthogonal to axes, in two dimensions this looks like the route a taxi would take around a grid system.  
It is an un-normalised form of Percentage score difference (Section~\ref{section:percentage}).  
Requires squareness.  
Asymmetric, but can be symmetric by disregarding the sign.
 
Varies between 0 (full agreement) and $\sum{X} + \sum{Y}$ (total difference). Is often {\color{red}  check!} normalised by discarding the sign.
% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & umm, $\pm (\sum{X} + \sum{Y}) $ I think. \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& None \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & Yes (0-padding possible) \\ \hline
    
    % Is the method symmetric?
    Symmetry? & No (sign varies) \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
{\color{red} TODO: fix sum(X) into common notation (row/column sums)}
\subsubsection{Asymmetric}
$$
D_M(Y|X) = \sum_{i=1}^{R}{ (Y_i - X_i) }
$$
Where $R$ is the number of items in $X$ and $Y$, as per Notation above.

\subsubsection{Symmetric}
$$
D_M = \sum_{i=1}^{R}{ |Y_i - X_i| }
$$
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================















% =============================================================================
\section{Tversky Index}
\label{section:tverskyindex}
\subsection{Description}
Generalised form of Dice coefficient (itself identical to the Jaccard index and S\o renson/Czekanowski's similarity index, itself equal to one minus the Hellinger Distance and Bray Curtis dissimilarity) used in information retrieval.  
Asymmetric. 
Used for fingerprint matching.  
Range is $[0, 1]$.  
See \cite{tversky1977features}
% http://www.daylight.com/dayhtml/doc/theory/theory.finger.html

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [0, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& ? \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions &  \\ \hline
    
    % Is the method symmetric?
    Symmetry? & No \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
For 2x2 contingency tables:
$$
T(Y|X) = \frac{ f_{11} }{ \alpha f_{10} + \beta f_{01} + f_{11} }
$$
Where $\alpha$ and $\beta$ control the weightings of false positive or negative `classification'.  Equal to the Jaccard/Tanimoto index where $\alpha = \beta = 1$, equal to the Dice index if $\alpha = \beta = 0.5$.

TODO: non-2x2 form.
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================














% =============================================================================
\section{Fisher's Exact Test}
\label{section:fisherexact}
\subsection{Description}
Designed for 2 $\times$ 2 matrices, though one is available for $2\times 3$ tables too (may be repeated with $k$ categories as with other methods with this restriction.).
% Wikipedia: However the principle of the test can be extended to the general case of an m Ã— n table,[7][8] and some statistical packages provide a calculation (sometimes using a Monte Carlo method to obtain an approximation)
% [7] -> Mehta C.R., Patel N.R. (1983). "A Network Algorithm for Performing Fisher's Exact Test in r Xc Contingency Tables". Journal of the American Statistical Association
% [8] -> http://mathworld.wolfram.com/FishersExactTest.html
May be hard to compute for large values due to its enumeration of combinatoric explosion.  
Suitable for small values.  
Used for categorical data.  
Tests for independence, similar to $\chi^2$ (only exact). 
Introduced by Fisher in 1922~\cite{fisher1922interpretation}.

% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & [0, 1] \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions&  \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Yes \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & No \\ \hline
    
    % Is the method symmetric?
    Symmetry? & Yes \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
% http://srmo.sagepub.com/view/the-sage-dictionary-of-statistics/n213.xml
In the $2\times 2$ form, it is computed as:
$$
p = \frac{ r_1!r_2!c_1!c_2! }{ f_{11}!f_{12}!f_{21}!f_{22}!S! }
$$

Generalised to $R\times C$, this becomes:
$$
p = \frac{ (\prod_{i=1}^{R}{ r_i! })  (\prod_{j=1}^{C}{ c_j! }) }{ S! \prod_{i=1}^{R}{  \prod_{j=1}^{C}{ f_{ij}! }}    }
$$
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================












% =============================================================================
\section{Kilgarriff's `Simple Maths'}
\subsection{Description}
A ratio of normalised frequency with a minimum bound, $\alpha$ defined.  
Where $\alpha$ is 0, is defined only for square input (nonzero).  
Range is $[0, \infty)$.  
Designed for two lists (the form used here) with frequencies set to 0 where lengths differ (overestimates differences where token type count not equal).


% Describe the rationale behind the measure:
%  1) Who developed it, when?
%  2) Why? What field was it originally developed for?
%  3) To solve what problem?

\subsection{Properties}
\begin{tabular}{| l || l |}
    \hline
    {\bf Property} & {\bf Description} \\
    \hline
    % What values may the statistic take
    Limits & $[0, \alpha]$ \\ \hline

    % Is this method only valid under certain
    % distributional assumptions?
    Distributional Assumptions& None \\ \hline

    % What types of data may this run on?
    Data type & nominal \\ \hline

    % Is the method suitable for small frequencies?
    Low Frequency & Sensitivity $\propto \alpha$ \\ \hline

    % Is the method suitable for nonsquare comparisons?
    Squareness Assumptions & Yes (0-padding possible) \\ \hline
    
    % Is the method symmetric?
    Symmetry? & ? \\ \hline

    % Finally, any other assumptions
    % x & x \\ \hline
\end{tabular}


\subsection{Formula}
For the `$2\times 2$' case (\textsl{i.e.} a single item $i$), as defined in \cite{kilgarriff2009simple}, \cite{kilgarriff2012getting}:
$$
K_{\alpha_i} = \frac{ M_{X_i} + \alpha }{ M_{Y_i} + \alpha }
$$
Where $M_{X_i}$ is the normalised frequency (per million) for item $i$ in list $X$:
$$
M_{X_i} = 1,000,000\frac{ X_i }{ R }
$$
Similarly for $Y$:
$$
M_{Y_i}= 1,000,000\frac{ Y_i }{ C }
$$

Where $\alpha$ is the saturation parameter used to define which frequencies are of maximal interest.

For the full list (as in presentation for \cite{kilgarriff2012getting}):
$$
K_{\alpha} = \frac{ \sum_{i \cup j}{ K_{\alpha_i} } }{ xc } 
$$
% How do we compute the statistic?

\subsection{Intrepretation}
% How to intrepret the output: what does it mean?
%  1) What range of values does it output?
%  2) In what circumstances does/can it hit extremis?
%  3) What does the number mean re. the data?

\subsection{Critical Values}
% Is it possible to find critical values for this measure?
%  1) If so, how? (brief mention is adequate for now)
%  2) In what contexts are the values valid?
% =============================================================================












% % =============================================================================
% %                               <template>
% % =============================================================================
% \section{Template} %name of group of statistics
% \subsection{Description}
% % Describe the rationale behind the measure:
% %  1) Who developed it, when?
% %  2) Why? What field was it originally developed for?
% %  3) To solve what problem?
% 
% \subsection{Properties}
% \begin{tabular}{| l || l |}
%     \hline
%     {\bf Property} & {\bf Description} \\
%     \hline
%     % What values may the statistic take
%     Limits & [-1, 1] \\ \hline
% 
%     % Is this method only valid under certain
%     % distributional assumptions?
%     Distributional Assumptions& 2 \\ \hline
% 
%     % What types of data may this run on?
%     Data type & nominal, ordinal, interval, ratio \\ \hline
% 
%     % Is the method suitable for small frequencies?
%     Low Frequency & 8 \\ \hline
% 
%     % Is the method suitable for nonsquare comparisons?
%     Squareness Assumptions & 8 \\ \hline
%     
%     % Is the method symmetric?
%     Symmetry? & Yes \\ \hline
% 
%     % Finally, any other assumptions
%     % x & x \\ \hline
% \end{tabular}
% 
% 
% \subsection{Formula}
% % How do we compute the statistic?
% 
% \subsection{Intrepretation}
% % How to intrepret the output: what does it mean?
% %  1) What range of values does it output?
% %  2) In what circumstances does/can it hit extremis?
% %  3) What does the number mean re. the data?
% 
% \subsection{Critical Values}
% % Is it possible to find critical values for this measure?
% %  1) If so, how? (brief mention is adequate for now)
% %  2) In what contexts are the values valid?
% % =============================================================================

\appendix

\section{Deliberate Omissions}
Some methods were omitted from the above list, yet still warrant a mention due to their relation to the above.  They are presented here primarily for justification as to their unsuitability:
\begin{itemize}
    \item \textbf{Hamming Distance}---Is unsuitable for use on frequency lists due to its requirement that data be ordered meaningfully.
    \item \textbf{SimRank}---Requires a graph model of many corpora to be of any use.  Might be applicable if considering texts within a corpus.
    \item \textbf{Linear-by-Linear}---Requires ordinal data
    %\item \textbf{}---
    % \item Mcnemar's test, for binary response, could be extended though.  Is this just another way of computing chi-sq statistics?  Used in the Transmission disequilibrium test in genetics.
\end{itemize}




\section{Table of Symbols}
\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Symbol} & \textbf{Comparison Method} \\ \hline
$\rho$              & Pearson's rho \\ \hline
$\rho_S$            & Spearman's rho \\ \hline
$\tau_a$            & Kendall's Tau-a \\ \hline
$\tau_b$            & Kendall's Tau-b \\ \hline
$\tau_c$            & Stuart's Tau-c \\ \hline
$\gamma$, G         & Goodman \& Kruskal's gamma\\ \hline
$\kappa$            & Cohen's kappa \\ \hline
$\kappa_F$          & Fleiss' kappa \\ \hline
$\lambda$           & Goodman \& Kruskal's lambda\\ \hline
$U$                 & Theil's U/Uncertainty coefficient\\ \hline
$\tau_{GK}$         & Goodman \& Kruskal's tau\\ \hline
$D_{KL}$            & Kullback-Leibler divergence\\ \hline
$D$                 & Somers' D\\ \hline
$\eta$              & Eta \\ \hline
$\chi^2$            & Pearson's chi-squared\\ \hline
$\phi$              & Matthews correlation coefficient\\ \hline
$V$                 & Cram\'er's V\\ \hline
$CC$                & Contingency coefficient \\ \hline
$OR$                & Odds ratio\\ \hline
$\alpha$            & Cronbach's alpha\\ \hline
$T$                 & Tschuprow's T\\ \hline
$F_\beta$           & F-score (where $\beta$ is the weighting parameter)\\ \hline
$U_{MW}$            & Mann-Whitney-Wilcoxon U\\ \hline
$D_{KS}$            & Kolmogorov-Smirnov D\\ \hline
$\Lambda$           & Likelihood ratio\\ \hline
$cos$               & Cosine Similarity\\ \hline
$D_\%$              & Percentage difference \\ \hline
$D_E$               & Euclidean distance\\ \hline
$D_M$               & Manhattan (taxicab) distance\\ \hline
$T_v$               & Tversky index\\ \hline
$p$                 & Fisher's exact test score (simple probabilty)\\ \hline
$K_{\alpha}$        & Kilgarriff's simple maths (where $\alpha$ is the saturation parameter)\\ \hline
%$ $                & \\ \hline
\end{tabular}
\caption{Symbols for each method}
\label{tab:symbols}
\end{table}



\section{Comparison}
\begin{landscape}
\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline

% TODO

\end{tabular}
\caption{Symbols for each method}
\label{tab:symbols}
\end{table}

\end{landscape}



% 'The needless assumption of normality in Pearson's r' http://psycnet.apa.org/journals/amp/12/10/623/


% =============================================================================
% =============================================================================
% =============================================================================

% =============================================================================
\bibliography{comparison}
\bibliographystyle{plain}
\end{document}
